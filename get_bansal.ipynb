{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Papers from Mohit Bansal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import workspace\n",
    "from scholar_api import *\n",
    "from gpt import rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_ID = '143977268'\n",
    "\n",
    "PROMPT = '''\n",
    "We're writing a paper about a prompt manager that integrates an LLM into a complex system. The LLM directs the user interaction with the system. The LLM serves as the interface between the user and the system. I'm looking for related works, e.g., LLM as a controller, as a user interfaces, as a tools director, or as an assistant.\n",
    "\n",
    "Evaluate the relevance of the following paper to my research.\n",
    "\n",
    "<paper>\n",
    "%s\n",
    "</paper>\n",
    "\n",
    "Is the above paper very relevant? Be strict and don't admit loosely-related works. Answer \"Yes\" or \"No\", using exactly one single word.\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = getPapersFromAuthor(AUTHOR_ID)\n",
    "len(papers), papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('citers.pickle', 'rb') as f:\n",
    "    citers = pickle.load(f)\n",
    "citers_set = set([x[PAPERID] for x in citers])\n",
    "papers_new = [x for x in papers if x[PAPERID] not in citers_set]\n",
    "len(papers_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in tqdm(papers_new):\n",
    "    paper_id = paper[PAPERID]\n",
    "    title = paper[TITLE] or 'None'\n",
    "    abstract = paper[ABSTRACT] or 'None'\n",
    "    if paper_id in rated:\n",
    "        continue\n",
    "    print()\n",
    "    print(title)\n",
    "    score = rate(PROMPT % (title + '\\nAbstract: ' + abstract).strip())\n",
    "    relevance = format(score, '.3%')\n",
    "    print(f'{relevance = }')\n",
    "    rated[paper_id] = (title, abstract, score, relevance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [*rated.items()]\n",
    "papers.sort(key=lambda x: x[1][2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([x[1][2] for x in papers], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.csv', 'w', encoding='utf-8', newline='') as fcsv:\n",
    "    c = csv.writer(fcsv)\n",
    "    for paper_id, (title, abstract, score, relevance) in papers:\n",
    "        print(relevance)\n",
    "        print(title)\n",
    "        print(abstract)\n",
    "        print()\n",
    "\n",
    "        c.writerow((relevance, title, abstract, paper_id))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
