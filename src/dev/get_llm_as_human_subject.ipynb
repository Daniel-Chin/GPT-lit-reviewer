{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Papers about LLM as Human Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scholar_api import *\n",
    "from gpt import rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '0ffd57884d7957f6b5634b9fa24843dc3759668f',\n",
       "  'title': 'Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study'},\n",
       " {'paperId': '5af9cf0b695faf2eb94d74bf76dab1a311638ca3',\n",
       "  'title': 'Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science'},\n",
       " {'paperId': 'cfade85a5a4bfd58ade304ea63ccfcf438898a14',\n",
       "  'title': 'Neural Language Models as What If? -Engines for HCI Research'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = searchPaper('Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_ID = ( # can be arxiv ID, DOI, or Semantic Scholar ID\n",
    "    '0ffd57884d7957f6b5634b9fa24843dc3759668f'\n",
    ")\n",
    "\n",
    "PROMPT = '''\n",
    "My idea: use an LLM agent as a proxy to the human subject in an HCI/psychology study. For example, the LLM agent can interact with a digital/virtual product, and reports the usage experience.\n",
    "\n",
    "Evaluate the relevance of the following paper to my idea. We already know the paper is probably about LLM, so only admit it if the relevance goes beyond topic match.\n",
    "\n",
    "<paper>\n",
    "%s\n",
    "</paper>\n",
    "\n",
    "Is the above paper very relevant? Answer \"Yes\" or \"No\", using exactly one single word.\n",
    "'''.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63,\n",
       " {'paperId': '36f7b5012ae8193b210a788e0a3f6845bf656e77',\n",
       "  'title': 'Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing',\n",
       "  'abstract': 'With the widespread popularity of internet celebrity marketing all over the world, short video production has gradually become a popular way of presenting products information. However, the traditional video production industry usually includes series of procedures as script writing, video filming in a professional studio, video clipping, special effects rendering, customized post-processing, and so forth. Not to mention that multilingual videos is not accessible for those who could not speak multilingual languages. These complicated procedures usually needs a professional team to complete, and this made short video production costly in both time and money. This paper presents an intelligent system that supports the automatic generation of talking avatar videos, namely Virbo. With simply a user-specified script, Virbo could use a deep generative model to generate a target talking videos. Meanwhile, the system also supports multimodal inputs to customize the video with specified face, specified voice and special effects. This system also integrated a multilingual customization module that supports generate multilingual talking avatar videos in a batch with hundreds of delicate templates and creative special effects. Through a series of user studies and demo tests, we found that Virbo can generate talking avatar videos that maintained a high quality of videos as those from a professional team while reducing the entire production costs significantly. This intelligent system will effectively promote the video production industry and facilitate the internet marketing neglecting of language barriers and cost challenges.'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = getPapersThatCite(PAPER_ID)\n",
    "len(papers), papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_new = papers\n",
    "len(papers_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing\n",
      "Creating OpenAI client...\n",
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/63 [00:01<02:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "Who Determines What Is Relevant? Humans or AI? Why Not Both?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/63 [00:03<01:30,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.061%'\n",
      "\n",
      "A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/63 [00:04<01:18,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 4/63 [00:05<01:15,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.005%'\n",
      "\n",
      "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/63 [00:07<01:26,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '1.205%'\n",
      "\n",
      "Humans or LLMs as the Judge? A Study on Judgement Biases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 6/63 [00:08<01:19,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.027%'\n",
      "\n",
      "I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 7/63 [00:09<01:15,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.004%'\n",
      "\n",
      "Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8/63 [00:10<01:11,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.021%'\n",
      "\n",
      "ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 9/63 [00:12<01:19,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/63 [00:14<01:14,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Can Large Language Model Agents Simulate Human Trust Behaviors?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/63 [00:15<01:09,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.863%'\n",
      "\n",
      "Large language models cannot replace human participants because they cannot portray identity groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 12/63 [00:16<01:10,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '63.703%'\n",
      "\n",
      "Picturing the fictitious person: An exploratory study on the effect of images on user perceptions of AI-generated personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 13/63 [00:18<01:07,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.289%'\n",
      "\n",
      "Leveraging Large Models for Crafting Narrative Visualization: A Survey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 14/63 [00:19<01:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "The illusion of artificial inclusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 15/63 [00:21<01:09,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.960%'\n",
      "\n",
      "How to write a CHI paper (asking for a friend)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 16/63 [00:22<01:12,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "On the Interaction with Large Language Models for Web Accessibility: Implications and Challenges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 17/63 [00:23<01:06,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.004%'\n",
      "\n",
      "The next generation of machine learning for tracking adaptation texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 18/63 [00:25<00:59,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 19/63 [00:25<00:53,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.768%'\n",
      "\n",
      "DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 20/63 [00:27<00:51,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Generative AI for corpus approaches to discourse studies: A critical evaluation of ChatGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 21/63 [00:28<00:46,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "A Structured Narrative Prompt for Prompting Narratives from Large Language Models: Sentiment Assessment of ChatGPT-Generated Narratives and Real Tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 22/63 [00:29<00:54,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '8.036%'\n",
      "\n",
      "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 23/63 [00:32<01:03,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Can AI serve as a substitute for human subjects in software engineering research?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 24/63 [00:34<01:07,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '100.000%'\n",
      "\n",
      "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 25/63 [00:35<01:02,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '2.844%'\n",
      "\n",
      "Do LLMs exhibit human-like response biases? A case study in survey design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 26/63 [00:36<00:56,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.310%'\n",
      "\n",
      "An HCI-Centric Survey and Taxonomy of Human-Generative-AI Interactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 27/63 [00:38<00:51,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.351%'\n",
      "\n",
      "QualiGPT: GPT as an easy-to-use tool for qualitative coding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 28/63 [00:39<00:50,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.806%'\n",
      "\n",
      "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 29/63 [00:40<00:47,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Gender, Age, and Technology Education Influence the Adoption and Appropriation of LLMs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 30/63 [00:41<00:41,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Can GPT-4 Replicate Empirical Software Engineering Research?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 31/63 [00:42<00:37,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.002%'\n",
      "\n",
      "The Participatory Turn in AI Design: Theoretical Foundations and the Current State of Practice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 32/63 [00:43<00:36,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "User Experience Design Professionals' Perceptions of Generative Artificial Intelligence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 33/63 [00:44<00:34,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.735%'\n",
      "\n",
      "People's Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 34/63 [00:46<00:39,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.002%'\n",
      "\n",
      "Natural Language based Context Modeling and Reasoning for Ubiquitous Computing with Large Language Models: A Tutorial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 35/63 [00:48<00:38,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '98.901%'\n",
      "\n",
      "Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 36/63 [00:49<00:36,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "Writer-Defined AI Personas for On-Demand Feedback Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 37/63 [00:50<00:31,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '10.669%'\n",
      "\n",
      "\"With Great Power Comes Great Responsibility!\": Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 38/63 [00:51<00:31,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 39/63 [00:52<00:29,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.011%'\n",
      "\n",
      "An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 40/63 [00:56<00:42,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '92.630%'\n",
      "\n",
      "Synthetic Text Generation using Hypergraph Representations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 41/63 [00:57<00:36,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "Employing large language models in survey research\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 42/63 [00:58<00:33,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '2.518%'\n",
      "\n",
      "Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 43/63 [00:59<00:27,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.002%'\n",
      "\n",
      "ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 44/63 [01:00<00:25,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Detecting the corruption of online questionnaires by artificial intelligence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 45/63 [01:02<00:23,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.009%'\n",
      "\n",
      "S3: Social-network Simulation System with Large Language Model-Empowered Agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 46/63 [01:03<00:24,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '6.371%'\n",
      "\n",
      "Embedding Democratic Values into Social Media AIs via Societal Objective Functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 47/63 [01:06<00:28,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.003%'\n",
      "\n",
      "LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 48/63 [01:08<00:28,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.782%'\n",
      "\n",
      "Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 49/63 [01:09<00:22,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Preparing Future Designers for Human-AI Collaboration in Persona Creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 50/63 [01:10<00:19,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '2.033%'\n",
      "\n",
      "Adding guardrails to advanced chatbots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 51/63 [01:12<00:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.007%'\n",
      "\n",
      "Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 52/63 [01:12<00:13,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.025%'\n",
      "\n",
      "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 53/63 [01:14<00:13,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 54/63 [01:15<00:12,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.524%'\n",
      "\n",
      "Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 55/63 [01:17<00:10,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.076%'\n",
      "\n",
      "Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 56/63 [01:18<00:09,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.002%'\n",
      "\n",
      "Generative Agents: Interactive Simulacra of Human Behavior\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 57/63 [01:20<00:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '99.999%'\n",
      "\n",
      "AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 58/63 [01:21<00:06,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.001%'\n",
      "\n",
      "Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 59/63 [01:22<00:05,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '2.096%'\n",
      "\n",
      "An Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 60/63 [01:24<00:04,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '84.390%'\n",
      "\n",
      "Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 61/63 [01:25<00:02,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '89.331%'\n",
      "\n",
      "Measuring machine learning harms from 1 stereotypes requires understanding who is being 2 harmed by which errors in what ways 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 62/63 [01:26<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n",
      "\n",
      "Can innovative prompt engineering with ChatGPT address imbalances in machine learning datasets?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:27<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance = '0.000%'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for paper in tqdm(papers_new):\n",
    "    paper_id = paper[PAPERID]\n",
    "    title = paper[TITLE] or 'None'\n",
    "    abstract = paper[ABSTRACT] or 'None'\n",
    "    if paper_id in rated:\n",
    "        continue\n",
    "    print()\n",
    "    print(title)\n",
    "    score = rate(PROMPT % (title + '\\nAbstract: ' + abstract).strip())\n",
    "    relevance = format(score, '.3%')\n",
    "    print(f'{relevance = }')\n",
    "    rated[paper_id] = (title, abstract, score, relevance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [*rated.items()]\n",
    "papers.sort(key=lambda x: x[1][2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([48.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  2., 11.]),\n",
       " array([7.73442281e-08, 9.99999190e-02, 1.99999761e-01, 2.99999602e-01,\n",
       "        3.99999444e-01, 4.99999286e-01, 5.99999128e-01, 6.99998969e-01,\n",
       "        7.99998811e-01, 8.99998653e-01, 9.99998494e-01]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([x[1][2] for x in papers], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.000%\n",
      "Can AI serve as a substitute for human subjects in software engineering research?\n",
      "Research within sociotechnical domains, such as software engineering, fundamentally requires the human perspective. Nevertheless, traditional qualitative data collection methods suffer from difficulties in participant recruitment, scaling, and labor intensity. This vision paper proposes a novel approach to qualitative data collection in software engineering research by harnessing the capabilities of artificial intelligence (AI), especially large language models (LLMs) like ChatGPT and multimodal foundation models. We explore the potential of AI-generated synthetic text as an alternative source of qualitative data, discussing how LLMs can replicate human responses and behaviors in research settings. We discuss AI applications in emulating humans in interviews, focus groups, surveys, observational studies, and user evaluations. We discuss open problems and research opportunities to implement this vision. In the future, an integrated approach where both AI and human-generated data coexist will likely yield the most effective outcomes.\n",
      "\n",
      "99.999%\n",
      "Generative Agents: Interactive Simulacra of Human Behavior\n",
      "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.\n",
      "\n",
      "99.960%\n",
      "The illusion of artificial inclusion\n",
      "Human participants play a central role in the development of modern artificial intelligence (AI) technology, in psychological science, and in user research. Recent advances in generative AI have attracted growing interest to the possibility of replacing human participants in these domains with AI surrogates. We survey several such\"substitution proposals\"to better understand the arguments for and against substituting human participants with modern generative AI. Our scoping review indicates that the recent wave of these proposals is motivated by goals such as reducing the costs of research and development work and increasing the diversity of collected data. However, these proposals ignore and ultimately conflict with foundational values of work with human participants: representation, inclusion, and understanding. This paper critically examines the principles and goals underlying human participation to help chart out paths for future work that truly centers and empowers participants.\n",
      "\n",
      "99.863%\n",
      "Can Large Language Model Agents Simulate Human Trust Behaviors?\n",
      "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, particularly for GPT-4, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications of our discoveries for various scenarios where trust is paramount. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans.\n",
      "\n",
      "99.782%\n",
      "LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs\n",
      "LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers' abilities in these\"human computation algorithms,\"but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross-comparing their performances on sub-tasks) and (2) LLMs' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.\n",
      "\n",
      "99.768%\n",
      "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives\n",
      "Agent-based modeling and simulation has evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, examining their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions.\n",
      "\n",
      "99.524%\n",
      "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap\n",
      "The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with. While the versatile capabilities of these models ignite excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in downstream use cases can be satisfied by the given model (socio-technical gap). By drawing on lessons from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods with an acknowledgment of trade-offs between realism to socio-requirements and pragmatic costs to conduct the evaluation. By mapping HCI and current NLG evaluation methods, we identify opportunities for evaluation methods for LLMs to narrow the socio-technical gap and pose open questions.\n",
      "\n",
      "99.351%\n",
      "An HCI-Centric Survey and Taxonomy of Human-Generative-AI Interactions\n",
      "Generative AI (GenAI) has shown remarkable capabilities in generating diverse and realistic content across different formats like images, videos, and text. In Generative AI, human involvement is essential, thus HCI literature has investigated how to effectively create collaborations between humans and GenAI systems. However, the current literature lacks a comprehensive framework to better understand Human-GenAI Interactions, as the holistic aspects of human-centered GenAI systems are rarely analyzed systematically. In this paper, we present a survey of 291 papers, providing a novel taxonomy and analysis of Human-GenAI Interactions from both human and Gen-AI perspectives. The dimensions of design space include 1) Purposes of Using Generative AI, 2) Feedback from Models to Users, 3) Control from Users to Models, 4) Levels of Engagement, 5) Application Domains, and 6) Evaluation Strategies. Our work is also timely at the current development stage of GenAI, where the Human-GenAI interaction design is of paramount importance. We also highlight challenges and opportunities to guide the design of Gen-AI systems and interactions towards the future design of human-centered Generative AI applications.\n",
      "\n",
      "99.310%\n",
      "Do LLMs exhibit human-like response biases? A case study in survey design\n",
      "As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of\"prompts\"have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey\n",
      "\n",
      "98.901%\n",
      "Natural Language based Context Modeling and Reasoning for Ubiquitous Computing with Large Language Models: A Tutorial\n",
      "Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users' request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases--(1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner.\n",
      "\n",
      "92.630%\n",
      "An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents\n",
      "The development of believable, natural, and interactive digital artificial agents is a field of growing interest. Theoretical uncertainties and technical barriers present considerable challenges to the field, particularly with regards to developing agents that effectively simulate human emotions. Large language models (LLMs) might address these issues by tapping common patterns in situational appraisal. In three empirical experiments, this study tests the capabilities of LLMs to solve emotional intelligence tasks and to simulate emotions. It presents and evaluates a new chain-of-emotion architecture for emotion simulation within video games, based on psychological appraisal research. Results show that it outperforms standard LLM architectures on a range of user experience and content analysis metrics. This study therefore provides early evidence of how to construct and test affective agents based on cognitive processes represented in language models.\n",
      "\n",
      "89.331%\n",
      "Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial\n",
      "Large language models (LLMs) have become phenomenally surging 1 , since 2018 – two decades after introducing context-awareness into computing systems [1–4]. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning [5, 6]. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing ( LCaC ). In the LCaC paradigm, users’ requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users’ request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases – (1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner. Furthermore, we analyze several factors that might affect the performance of LLM-driven context-awareness, and then discuss the\n",
      "\n",
      "84.390%\n",
      "An Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox\n",
      "The “privacy paradox” describes the discrepancy between users’ privacy attitudes and their actual behaviors. Mitigating this discrepancy requires solutions that account for both system opaqueness and users’ hesitations in testing different privacy settings due to fears of unintended data exposure. We introduce an empathy-based approach that allows users to experience how privacy behaviors may alter system outcomes in a risk-free sandbox environment from the perspective of artificially generated personas. To generate realistic personas, we introduce a novel pipeline that augments the outputs of large language models using few-shot learning, contextualization, and chain of thoughts. Our empirical studies demonstrated the adequate quality of generated personas and highlighted the changes in privacy-related applications (e.g., online advertising) caused by different personas. Furthermore, users demonstrated cognitive and emotional empathy towards the personas when interacting with our sandbox. We offered design implications for downstream applications in improving user privacy literacy and promoting behavior changes.\n",
      "\n",
      "63.703%\n",
      "Large language models cannot replace human participants because they cannot portray identity groups\n",
      "Large language models (LLMs) are increasing in capability and popularity, propelling their application in new domains -- including as replacements for human participants in computational social science, user testing, annotation tasks, and more. Traditionally, in all of these settings survey distributors are careful to find representative samples of the human population to ensure the validity of their results and understand potential demographic differences. This means in order to be a suitable replacement, LLMs will need to be able to capture the influence of positionality (i.e., relevance of social identities like gender and race). However, we show that there are two inherent limitations in the way current LLMs are trained that prevent this. We argue analytically for why LLMs are doomed to both misportray and flatten the representations of demographic groups, then empirically show this to be true on 4 LLMs through a series of human studies with 3200 participants across 16 demographic identities. We also discuss a third consideration about how identity prompts can essentialize identities. Throughout, we connect each of these limitations to a pernicious history that shows why each is harmful for marginalized demographic groups. Overall, we urge caution in use cases where LLMs are intended to replace human participants whose identities are relevant to the task at hand. At the same time, in cases where the goal is to supplement rather than replace (e.g., pilot studies), we provide empirically-better inference-time techniques to reduce, but not remove, these harms.\n",
      "\n",
      "10.669%\n",
      "Writer-Defined AI Personas for On-Demand Feedback Generation\n",
      "Compelling writing is tailored to its audience. This is challenging, as writers may struggle to empathize with readers, get feedback in time, or gain access to the target group. We propose a concept that generates on-demand feedback, based on writer-defined AI personas of any target audience. We explore this concept with a prototype (using GPT-3.5) in two user studies (N=5 and N=11): Writers appreciated the concept and strategically used personas for getting different perspectives. The feedback was seen as helpful and inspired revisions of text and personas, although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.\n",
      "\n",
      "8.036%\n",
      "A Structured Narrative Prompt for Prompting Narratives from Large Language Models: Sentiment Assessment of ChatGPT-Generated Narratives and Real Tweets\n",
      "Large language models (LLMs) excel in providing natural language responses that sound authoritative, reflect knowledge of the context area, and can present from a range of varied perspectives. Agent-based models and simulations consist of simulated agents that interact within a simulated environment to explore societal, social, and ethical, among other, problems. Simulated agents generate large volumes of data and discerning useful and relevant content is an onerous task. LLMs can help in communicating agents’ perspectives on key life events by providing natural language narratives. However, these narratives should be factual, transparent, and reproducible. Therefore, we present a structured narrative prompt for sending queries to LLMs, we experiment with the narrative generation process using OpenAI’s ChatGPT, and we assess statistically significant differences across 11 Positive and Negative Affect Schedule (PANAS) sentiment levels between the generated narratives and real tweets using chi-squared tests and Fisher’s exact tests. The narrative prompt structure effectively yields narratives with the desired components from ChatGPT. In four out of forty-four categories, ChatGPT generated narratives which have sentiment scores that were not discernibly different, in terms of statistical significance (alpha level α=0.05), from the sentiment expressed in real tweets. Three outcomes are provided: (1) a list of benefits and challenges for LLMs in narrative generation; (2) a structured prompt for requesting narratives of an LLM chatbot based on simulated agents’ information; (3) an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets. This indicates significant promise in the utilization of LLMs for helping to connect a simulated agent’s experiences with real people.\n",
      "\n",
      "6.371%\n",
      "S3: Social-network Simulation System with Large Language Model-Empowered Agents\n",
      "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.\n",
      "\n",
      "2.844%\n",
      "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n",
      "Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.\n",
      "\n",
      "2.518%\n",
      "Employing large language models in survey research\n",
      "None\n",
      "\n",
      "2.096%\n",
      "Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering\n",
      "Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, the two LLMs are closed source, and little is known about the LLMs’ performance in real-world use cases. In academia, LLM performance is often measured on benchmarks which may have leaked into ChatGPT’s and GPT-4’s training data. In this paper, we apply and evaluate ChatGPT and GPT-4 for the real-world task of cost-efficient extractive question answering over a text corpus that was published after the two LLMs completed training. More specifically, we extract research challenges for researchers in the field of HCI from the proceedings of the 2023 Conference on Human Factors in Computing Systems (CHI). We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for prototyping research ideas and analyzing text corpora from different perspectives, with implications for applying LLMs in academia and practice. For researchers in HCI, we contribute an interactive visualization of 4392 research challenges in over 90 research topics. We share this visualization and the dataset in the spirit of open science. 1\n",
      "\n",
      "2.033%\n",
      "Preparing Future Designers for Human-AI Collaboration in Persona Creation\n",
      "This paper presents findings from an exploratory study investigating the use of AI text-generation tools to support novice designers in persona creation. We conducted a workshop with 22 undergraduate students enrolled in an introductory human-computer interaction course, who were instructed to use GPT-3 in the creation of personas. These novice designers were able to use GPT-3 to iterate to produce satisfactory personas, particularly when providing detailed prompts. Our findings suggest that personas created with GPT-3 assistance were mostly comparable to those created manually but rated lower on some evaluation dimensions. The study also reveals merits and concerns of using GPT-3 for persona creation. Based on our findings, we propose recommendations for novice designers on how to use text-generative AIs to create personas effectively and responsibly.\n",
      "\n",
      "1.205%\n",
      "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation\n",
      "Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized\"trick tests\"and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned LLMs. For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises. We found no correspondence between trick tests and RUTEd evaluations. Specifically, selecting the least biased model based on the de-contextualized results coincides with selecting the model with the best performance on RUTEd evaluations only as often as random chance. We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.\n",
      "\n",
      "0.806%\n",
      "QualiGPT: GPT as an easy-to-use tool for qualitative coding\n",
      "Qualitative research delves deeply into individual complex perspectives on technology and various phenomena. However, a meticulous analysis of qualitative data often requires a significant amount of time, especially during the crucial coding stage. Although there is software specifically designed for qualitative evaluation, many of these platforms fall short in terms of automatic coding, intuitive usability, and cost-effectiveness. With the rise of Large Language Models (LLMs) such as GPT-3 and its successors, we are at the forefront of a transformative era for enhancing qualitative analysis. In this paper, we introduce QualiGPT, a specialized tool designed after considering challenges associated with ChatGPT and qualitative analysis. It harnesses the capabilities of the Generative Pretrained Transformer (GPT) and its API for thematic analysis of qualitative data. By comparing traditional manual coding with QualiGPT's analysis on both simulated and actual datasets, we verify that QualiGPT not only refines the qualitative analysis process but also elevates its transparency, credibility, and accessibility. Notably, compared to existing analytical platforms, QualiGPT stands out with its intuitive design, significantly reducing the learning curve and operational barriers for users.\n",
      "\n",
      "0.735%\n",
      "User Experience Design Professionals' Perceptions of Generative Artificial Intelligence\n",
      "Among creative professionals, Generative Artificial Intelligence (GenAI) has sparked excitement over its capabilities and fear over unanticipated consequences. How does GenAI impact User Experience Design (UXD) practice, and are fears warranted? We interviewed 20 UX Designers, with diverse experience and across companies (startups to large enterprises). We probed them to characterize their practices, and sample their attitudes, concerns, and expectations. We found that experienced designers are confident in their originality, creativity, and empathic skills, and find GenAI's role as assistive. They emphasized the unique human factors of\"enjoyment\"and\"agency\", where humans remain the arbiters of\"AI alignment\". However, skill degradation, job replacement, and creativity exhaustion can adversely impact junior designers. We discuss implications for human-GenAI collaboration, specifically copyright and ownership, human creativity and agency, and AI literacy and access. Through the lens of responsible and participatory AI, we contribute a deeper understanding of GenAI fears and opportunities for UXD.\n",
      "\n",
      "0.289%\n",
      "Picturing the fictitious person: An exploratory study on the effect of images on user perceptions of AI-generated personas\n",
      "None\n",
      "\n",
      "0.076%\n",
      "Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions\n",
      "The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.\n",
      "\n",
      "0.061%\n",
      "Who Determines What Is Relevant? Humans or AI? Why Not Both?\n",
      "None\n",
      "\n",
      "0.027%\n",
      "Humans or LLMs as the Judge? A Study on Judgement Biases\n",
      "Adopting human and large language models (LLM) as judges (\\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of developing robust evaluation systems.\n",
      "\n",
      "0.025%\n",
      "Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale\n",
      "Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, these LLMs are closed source, and little is known about their performance in real-world use cases. In this paper, we apply and evaluate the combination of ChatGPT and GPT-4 for the real-world task of mining insights from a text corpus in order to identify research challenges in the field of HCI. We extract 4,392 research challenges in over 100 topics from the 2023 CHI conference proceedings and visualize the research challenges for interactive exploration. We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for flexibly prototyping research ideas and analyzing text corpora from different perspectives, with implications for applying LLMs for mining insights in academia and practice.\n",
      "\n",
      "0.021%\n",
      "Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem\n",
      "Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models to propose levels based on the gameplay data continuously collected from individual players. We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques. Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level.\n",
      "\n",
      "0.011%\n",
      "Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model\n",
      "Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.\n",
      "\n",
      "0.009%\n",
      "Detecting the corruption of online questionnaires by artificial intelligence\n",
      "Online questionnaires that use crowdsourcing platforms to recruit participants have become commonplace, due to their ease of use and low costs. Artificial intelligence (AI)-based large language models (LLMs) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. These technological advances threaten the data quality for studies that use online questionnaires. This study tested whether text generated by an AI for the purpose of an online study can be detected by both humans and automatic AI detection systems. While humans were able to correctly identify the authorship of such text above chance level (76% accuracy), their performance was still below what would be required to ensure satisfactory data quality. Researchers currently have to rely on a lack of interest among bad actors to successfully use open-ended responses as a useful tool for ensuring data quality. Automatic AI detection systems are currently completely unusable. If AI submissions of responses become too prevalent, then the costs associated with detecting fraudulent submissions will outweigh the benefits of online questionnaires. Individual attention checks will no longer be a sufficient tool to ensure good data quality. This problem can only be systematically addressed by crowdsourcing platforms. They cannot rely on automatic AI detection systems and it is unclear how they can ensure data quality for their paying clients.\n",
      "\n",
      "0.007%\n",
      "Adding guardrails to advanced chatbots\n",
      "Generative AI models continue to become more powerful. The launch of ChatGPT in November 2022 has ushered in a new era of AI. ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. There are already concerns that humans may be replaced by chatbots for a variety of jobs. Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. These biases may cause significant harm and/or inequity toward different subpopulations. To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement. We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. We find that ChatGPT is very sensitive to changes in the prompt, where small changes lead to different levels of fairness. This suggests that we need to immediately implement\"corrections\"or mitigation strategies in order to improve fairness of these systems. We suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate.\n",
      "\n",
      "0.005%\n",
      "A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education\n",
      "Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment. We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up.\n",
      "\n",
      "0.004%\n",
      "I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models\n",
      "We explored cultural biases-individualism vs. collectivism-in ChatGPT across three Western languages (i.e., English, German, and French) and three Eastern languages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an individualistic persona in Western languages, its collectivism scores (i.e., out-group values) exhibited a more negative trend, surpassing their positive orientation towards individualism (i.e., in-group values). Conversely, when a collectivistic persona was assigned to ChatGPT in Eastern languages, a similar pattern emerged with more negative responses toward individualism (i.e., out-group values) as compared to collectivism (i.e., in-group values). The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group. The experiment was replicated in the political domain, and the results remained consistent. Furthermore, this replication unveiled an intrinsic Democratic bias in Large Language Models (LLMs), aligning with earlier findings and providing integral insights into mitigating such bias through prompt engineering. Extensive robustness checks were performed using varying hyperparameter and persona setup methods, with or without social identity labels, across other popular language models.\n",
      "\n",
      "0.004%\n",
      "On the Interaction with Large Language Models for Web Accessibility: Implications and Challenges\n",
      "The widespread diffusion of Large Language Models (LLMs) has ushered in a transformative era across numerous research domains, including web accessibility. In fact, they can potentially offer automated solutions for generating accessible content, performing accessibility testing, and enhancing the overall user experience for individuals with disabilities. In this paper, we investigate how LLMs can be successfully employed to evaluate and correct web accessibility. Then, we delve into the positive implications and the current challenges derived from the interaction between developers and LLMs in this specific context. Finally, we present some future directions that could be explored to ensure that web content remains accessible to all.\n",
      "\n",
      "0.003%\n",
      "Embedding Democratic Values into Social Media AIs via Societal Objective Functions\n",
      "Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs.\n",
      "\n",
      "0.002%\n",
      "Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages\n",
      "Large language models (LLMs) are increasingly capable and prevalent, and can be used to produce creative content. The quality of content is influenced by the prompt used, with more specific prompts that incorporate examples generally producing better results. On from this, it could be seen that using instructions written for crowdsourcing tasks (that are specific and include examples to guide workers) could prove effective LLM prompts. To explore this, we used a previous crowdsourcing pipeline that gave examples to people to help them generate a collectively diverse corpus of motivational messages. We then used this same pipeline to generate messages using GPT-4, and compared the collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the pipeline, and (3 & 4) two baseline GPT-4 prompts. We found that the LLM prompts using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages than the two baseline prompts. We also discuss implications from messages generated by both human writers and LLMs.\n",
      "\n",
      "0.002%\n",
      "People's Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review\n",
      "Large language models (LLMs) have brought breakthroughs in tasks including translation, summarization, information retrieval, and language generation, gaining growing interest in the CHI community. Meanwhile, the literature shows researchers' controversial perceptions about the efficacy, ethics, and intellectual abilities of LLMs. However, we do not know how people perceive LLMs that are pervasive in everyday tools, specifically regarding their experience with LLMs around bias, stereotypes, social norms, or safety. In this study, we conducted a systematic review to understand what empirical insights papers have gathered about people's perceptions toward LLMs. From a total of 231 retrieved papers, we full-text reviewed 15 papers that recruited human evaluators to assess their experiences with LLMs. We report different biases and related concepts investigated by these studies, four broader LLM application areas, the evaluators' perceptions toward LLMs' performances including advantages, biases, and conflicting perceptions, factors influencing these perceptions, and concerns about LLM applications.\n",
      "\n",
      "0.002%\n",
      "Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities\n",
      "Virtual assistants are becoming increasingly important speech-driven Information Retrieval platforms that assist users with various tasks. We discuss open problems and challenges with respect to modeling spoken information queries for virtual assistants, and list opportunities where Information Retrieval methods and research can be applied to improve the quality of virtual assistant speech recognition. We discuss how query domain classification, knowledge graphs and user interaction data, and query personalization can be helpful to improve the accurate recognition of spoken information domain queries. Finally, we also provide a brief overview of current problems and challenges in speech recognition.\n",
      "\n",
      "0.002%\n",
      "Can GPT-4 Replicate Empirical Software Engineering Research?\n",
      "Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help democratize empirical software engineering research. In this paper, we examine LLMs' abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.\n",
      "\n",
      "0.001%\n",
      "Synthetic Text Generation using Hypergraph Representations\n",
      "Generating synthetic variants of a document is often posed as text-to-text transformation. We propose an alternate LLM based method that first decomposes a document into semantic frames and then generates text using this interim sparse format. The frames are modeled using a hypergraph, which allows perturbing the frame contents in a principled manner. Specifically, new hyperedges are mined through topological analysis and complex polyadic relationships including hierarchy and temporal dynamics are accommodated. We show that our solution generates documents that are diverse, coherent and vary in style, sentiment, format, composition and facts.\n",
      "\n",
      "0.001%\n",
      "Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing\n",
      "With the widespread popularity of internet celebrity marketing all over the world, short video production has gradually become a popular way of presenting products information. However, the traditional video production industry usually includes series of procedures as script writing, video filming in a professional studio, video clipping, special effects rendering, customized post-processing, and so forth. Not to mention that multilingual videos is not accessible for those who could not speak multilingual languages. These complicated procedures usually needs a professional team to complete, and this made short video production costly in both time and money. This paper presents an intelligent system that supports the automatic generation of talking avatar videos, namely Virbo. With simply a user-specified script, Virbo could use a deep generative model to generate a target talking videos. Meanwhile, the system also supports multimodal inputs to customize the video with specified face, specified voice and special effects. This system also integrated a multilingual customization module that supports generate multilingual talking avatar videos in a batch with hundreds of delicate templates and creative special effects. Through a series of user studies and demo tests, we found that Virbo can generate talking avatar videos that maintained a high quality of videos as those from a professional team while reducing the entire production costs significantly. This intelligent system will effectively promote the video production industry and facilitate the internet marketing neglecting of language barriers and cost challenges.\n",
      "\n",
      "0.001%\n",
      "Generative AI for corpus approaches to discourse studies: A critical evaluation of ChatGPT\n",
      "None\n",
      "\n",
      "0.001%\n",
      "How to write a CHI paper (asking for a friend)\n",
      "Writing and genre conventions are extant to any scientific community, and CHI is no different. In this paper, we present the early phases of an AI tool we created called KITSUNE, which supports authors in placing their work into the format of a CHI paper, taking into account many conventions that are ever-present in CHI papers. We describe the development of the tool with the intent to promote discussion around how writing conventions are upheld and unquestioned by the CHI community, and how this translates to the work produced. In addition, we bring up questions surrounding how the introduction of LLMs into academic writing fundamentally change how conventions will be upheld now and in the future\n",
      "\n",
      "0.001%\n",
      "A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation\n",
      "Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs). Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process. This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage. Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content.\n",
      "\n",
      "0.001%\n",
      "AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys\n",
      "How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs ﬁne-tuned by nationally representative surveys for opinion prediction – missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and ρ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, ρ = 0.98). These remarkable prediction capabilities allow us to ﬁll in missing trends with high conﬁdence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zero-shot prediction task (AUC = 0.73, ρ = 0.67), highlighting challenges presented by LLMs without human responses. Further, we ﬁnd that the best models’ accuracy is lower for individuals with low socioeconomic status, racial minorities, and non-partisan afﬁliations but higher for ideologically sorted opinions in contemporary periods. We discuss practical constraints, socio-demographic representation, and ethical concerns regarding individual autonomy and privacy when using LLMs for opinion prediction. This paper showcases a new approach for leveraging LLMs to enhance nationally representative surveys by predicting missing responses and trends.\n",
      "\n",
      "0.001%\n",
      "Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models\n",
      "We introduce VL2NL, a Large Language Model (LLM) framework that generates rich and diverse NL datasets using only Vega-Lite specifications as input, thereby streamlining the development of Natural Language Interfaces (NLIs) for data visualization. To synthesize relevant chart semantics accurately and enhance syntactic diversity in each NL dataset, we leverage 1) a guided discovery incorporated into prompting so that LLMs can steer themselves to create faithful NL datasets in a self-directed manner; 2) a score-based paraphrasing to augment NL syntax along with four language axes. We also present a new collection of 1,981 real-world Vega-Lite specifications that have increased diversity and complexity than existing chart collections. When tested on our chart collection, VL2NL extracted chart semantics and generated L1/L2 captions with 89.4% and 76.0% accuracy, respectively. It also demonstrated generating and paraphrasing utterances and questions with greater diversity compared to the benchmarks. Last, we discuss how our NL datasets and framework can be utilized in real-world scenarios. The codes and chart collection are available at https://github.com/hyungkwonko/chart-llm.\n",
      "\n",
      "0.001%\n",
      "ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions\n",
      "Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers' queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI. Two months after ChatGPT's release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them. This way, we aim to (ii) measure user engagement evolution with Stack Overflow over time; (ii) quantify the reliability of LLMs' answers and their potential to replace Stack Overflow in the long term; (iii) identify and understand why LLMs fails; and (iv) compare LLMs together. Our empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. Furthermore, we also discuss the impact of our findings regarding the usage and development of new LLMs.\n",
      "\n",
      "0.000%\n",
      "ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations\n",
      "Background ChatGPT has emerged as a valuable tool for enhancing scientific writing. It is the first openly available Large Language Model (LLM) with unrestricted access to its capabilities. ChatGPT has the potential to alleviate researchers’ workload and enhance various aspects of research, from planning to execution and presentation. However, due to the rapid growth of publications and diverse opinions surrounding ChatGPT, a comprehensive review is necessary to understand its benefits, risks, and safe utilization in scientific research. This review aims to provide a comprehensive overview of the topic by extensively examining existing literature on the utilization of ChatGPT in academic research. The goal is to gain insights into the potential benefits and risks of using ChatGPT in scientific research, exploring secure and efficient methods for its application while identifying potential pitfalls to minimize negative consequences. Method The search was conducted in PubMed/MEDLINE, SCOPUS, and Google Scholar, yielding a total of 1279 articles and concluded on April 23rd, 2023. After full screening of titles/abstracts and removing duplicates and irrelevant articles, a total of 181 articles were included for analysis. Information collected included publication details, purposes, benefits, risks, and recommendation regarding ChatGPT’s use in scientific research. Results The majority of existing literature consists of editorials expressing thoughts and concerns, followed by original research articles analyzing ChatGPT’s performance in scientific research. The most significant advantage of using ChatGPT in scientific writing is its ability to expedite the writing process, enabling researchers to draft their work more efficiently. It also proves beneficial in improving writing style and proofreading by offering suggestions for sentence structure, grammar, and overall clarity. Additional benefits identified include support in data analysis, the formulation of protocols for clinical trials, and the design of scientific studies. Concerns mainly revolve around the accuracy and superficiality of the generated content, leading to what is referred to as “hallucinations.” Researchers have also expressed concerns about the tool providing citations to nonexistent sources. Other concerns discussed include authorship and plagiarism issues, accountability, copyright considerations, potential loss of diverse writing styles, privacy and security, transparency, credibility, validity, presence of bias, and the potential impact on scientific progress, such as a decrease in groundbreaking discoveries. Conclusion ChatGPT has the potential to revolutionize scientific writing as a valuable tool for researchers. However, it cannot replace human expertise and critical thinking. Researchers must exercise caution, ensuring the generated content complements their own knowledge. Ethical standards should be upheld, involving knowledgeable human researchers to avoid biases and inaccuracies. Collaboration among stakeholders and training on AI technology are essential for identifying best practices in LLMs use and maintaining scientific integrity.\n",
      "\n",
      "0.000%\n",
      "\"With Great Power Comes Great Responsibility!\": Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education\n",
      "The rise in popularity of Large Language Models (LLMs) has prompted discussions in academic circles, with students exploring LLM-based tools for coursework inquiries and instructors exploring them for teaching and research. Even though a lot of work is underway to create LLM-based tools tailored for students and instructors, there is a lack of comprehensive user studies that capture the perspectives of students and instructors regarding LLMs. This paper addresses this gap by conducting surveys and interviews within undergraduate engineering universities in India. Using 1306 survey responses among students, 112 student interviews, and 27 instructor interviews around the academic usage of ChatGPT (a popular LLM), this paper offers insights into the current usage patterns, perceived benefits, threats, and challenges, as well as recommendations for enhancing the adoption of LLMs among students and instructors. These insights are further utilized to discuss the practical implications of LLMs in undergraduate engineering education and beyond.\n",
      "\n",
      "0.000%\n",
      "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing\n",
      "Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the\"better\"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.\n",
      "\n",
      "0.000%\n",
      "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation\n",
      "GPT (Generative Pre-trained Transformer) represents advanced language models that have significantly reshaped the academic writing landscape. These sophisticated language models offer invaluable support throughout all phases of research work, facilitating idea generation, enhancing drafting processes, and overcoming challenges like writer’s block. Their capabilities extend beyond conventional applications, contributing to critical analysis, data augmentation, and research design, thereby elevating the efficiency and quality of scholarly endeavors. Strategically narrowing its focus, this review explores alternative dimensions of GPT and LLM applications, specifically data augmentation and the generation of synthetic data for research. Employing a meticulous examination of 412 scholarly works, it distills a selection of 77 contributions addressing three critical research questions: (1) GPT on Generating Research data, (2) GPT on Data Analysis, and (3) GPT on Research Design. The systematic literature review adeptly highlights the central focus on data augmentation, encapsulating 48 pertinent scholarly contributions, and extends to the proactive role of GPT in critical analysis of research data and shaping research design. Pioneering a comprehensive classification framework for “GPT’s use on Research Data”, the study classifies existing literature into six categories and 14 sub-categories, providing profound insights into the multifaceted applications of GPT in research data. This study meticulously compares 54 pieces of literature, evaluating research domains, methodologies, and advantages and disadvantages, providing scholars with profound insights crucial for the seamless integration of GPT across diverse phases of their scholarly pursuits.\n",
      "\n",
      "0.000%\n",
      "Measuring machine learning harms from 1 stereotypes requires understanding who is being 2 harmed by which errors in what ways 3\n",
      "Abstract\n",
      "\n",
      "0.000%\n",
      "Gender, Age, and Technology Education Influence the Adoption and Appropriation of LLMs\n",
      "Large Language Models (LLMs) such as ChatGPT have become increasingly integrated into critical activities of daily life, raising concerns about equitable access and utilization across diverse demographics. This study investigates the usage of LLMs among 1,500 representative US citizens. Remarkably, 42% of participants reported utilizing an LLM. Our findings reveal a gender gap in LLM technology adoption (more male users than female users) with complex interaction patterns regarding age. Technology-related education eliminates the gender gap in our sample. Moreover, expert users are more likely than novices to list professional tasks as typical application scenarios, suggesting discrepancies in effective usage at the workplace. These results underscore the importance of providing education in artificial intelligence in our technology-driven society to promote equitable access to and benefits from LLMs. We urge for both international replication beyond the US and longitudinal observation of adoption.\n",
      "\n",
      "0.000%\n",
      "Leveraging Large Models for Crafting Narrative Visualization: A Survey\n",
      "Narrative visualization effectively transforms data into engaging stories, making complex information accessible to a broad audience. Large models, essential for narrative visualization, inherently facilitate this process through their superior ability to handle natural language queries and answers, generate cohesive narratives, and enhance visual communication. Inspired by previous work in narrative visualization and recent advances in large models, we synthesized potential tasks and opportunities for large models at various stages of narrative visualization. In our study, we surveyed 79 papers to explore the role of large models in automating narrative visualization creation. We propose a comprehensive pipeline that leverages large models for crafting narrative visualization, categorizing the reviewed literature into four essential phases: Data, Narration, Visualization, and Presentation. Additionally, we identify nine specific tasks where large models are applied across these stages. This study maps out the landscape of challenges and opportunities in the LM4NV process, providing insightful directions for future research and valuable guidance for scholars in the field.\n",
      "\n",
      "0.000%\n",
      "Can innovative prompt engineering with ChatGPT address imbalances in machine learning datasets?\n",
      "—Large language models are experiencing a significant surge of attention and rapid development. It is happening mainly due to the publication of OpenAI’s ChatGPT models: GPT3.5-turbo and GPT-4. This article uses prompt engineering to present an innovative approach to synthetic data generation and knowledge distillation. Specifically, we focus on three methods: basic prompts, composite prompts, and similarity prompts. This research aims to investigate the potential of these techniques to address the problem of unbalanced datasets, a common issue in machine learning applications. Experimental results reveal that none of the prompt-based strategies achieve scores on par with the entire dataset. However, the similarity prompts method shows promising potential, outperforming other approaches. The study suggests a significant opportunity to develop these techniques further to generate more diverse synthetic data. Although the results are preliminary, they open up exciting possibilities for future research in this area, including integrating more advanced versions of Large Language Models and exploring other machine learning domains.\n",
      "\n",
      "0.000%\n",
      "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\n",
      "The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.\n",
      "\n",
      "0.000%\n",
      "The Participatory Turn in AI Design: Theoretical Foundations and the Current State of Practice\n",
      "Despite the growing consensus that stakeholders affected by AI systems should participate in their design, enormous variation and implicit disagreements exist among current approaches. For researchers and practitioners who are interested in taking a participatory approach to AI design and development, it remains challenging to assess the extent to which any participatory approach grants substantive agency to stakeholders. This article thus aims to ground what we dub the “participatory turn” in AI design by synthesizing existing theoretical literature on participation and through empirical investigation and critique of its current practices. Specifically, we derive a conceptual framework through synthesis of literature across technology design, political theory, and the social sciences that researchers and practitioners can leverage to evaluate approaches to participation in AI design. Additionally, we articulate empirical findings concerning the current state of participatory practice in AI design based on an analysis of recently published research and semi-structured interviews with 12 AI researchers and practitioners. We use these empirical findings to understand the current state of participatory practice and subsequently provide guidance to better align participatory goals and methods in a way that accounts for practical constraints.\n",
      "\n",
      "0.000%\n",
      "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching\n",
      "Navigating drones through natural language commands remains challenging due to the dearth of accessible multi-modal datasets and the stringent precision requirements for aligning visual and textual data. To address this pressing need, we introduce GeoText-1652, a new natural language-guided geo-localization benchmark. This dataset is systematically constructed through an interactive human-computer process leveraging Large Language Model (LLM) driven annotation techniques in conjunction with pre-trained vision models. GeoText-1652 extends the established University-1652 image dataset with spatial-aware text annotations, thereby establishing one-to-one correspondences between image, text, and bounding box elements. We further introduce a new optimization objective to leverage fine-grained spatial associations, called blending spatial matching, for region-level spatial relation matching. Extensive experiments reveal that our approach maintains a competitive recall rate comparing other prevailing cross-modality methods. This underscores the promising potential of our approach in elevating drone control and navigation through the seamless integration of natural language commands in real-world scenarios.\n",
      "\n",
      "0.000%\n",
      "Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection\n",
      "ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data. However, this means that they also have the potential for misuse, specifically to automatically answer surveys. LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses to surveys. The mechanism uses\"prompt injection\", such as directions that can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 93% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs.\n",
      "\n",
      "0.000%\n",
      "DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content\n",
      "This paper explores the image synthesis capabilities of GPT-4, a leading multi-modal large language model. We establish a benchmark for evaluating the fidelity of texture features in images generated by GPT-4, comprising manually painted pictures and their AI-generated counterparts. The contributions of this study are threefold: First, we provide an in-depth analysis of the fidelity of image synthesis features based on GPT-4, marking the first such study on this state-of-the-art model. Second, the quantitative and qualitative experiments fully reveals the limitations of the GPT-4 model in image synthesis. Third, we have compiled a unique benchmark of manual drawings and corresponding GPT-4-generated images, introducing a new task to advance fidelity research in AI-generated content (AIGC). The dataset is available at: \\url{https://github.com/rickwang28574/DeepArt}.\n",
      "\n",
      "0.000%\n",
      "The next generation of machine learning for tracking adaptation texts\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('results.csv', 'w', encoding='utf-8', newline='') as fcsv:\n",
    "    c = csv.writer(fcsv)\n",
    "    for paper_id, (title, abstract, score, relevance) in papers:\n",
    "        print(relevance)\n",
    "        print(title)\n",
    "        print(abstract)\n",
    "        print()\n",
    "\n",
    "        c.writerow((relevance, title, abstract, paper_id))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
